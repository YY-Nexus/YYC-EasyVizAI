"""
å­¦ä¹ é—­ç¯è‡ªåŠ¨åŒ–æœåŠ¡
"""
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from django.conf import settings
from django.contrib.auth.models import User
from django.db.models import Count, Q
from django.utils import timezone
from django.template.loader import render_to_string

from app.core.models import (
    ChatMessage, ChatSession, LearningNode, LearningProgress, 
    LearningReport, KnowledgePoint, SemanticIndex
)
from app.core.search import search_service


class LearningDataCollector:
    """å­¦ä¹ æ•°æ®æ”¶é›†å™¨"""
    
    @staticmethod
    def collect_chat_interactions(user_id: int, days: int = 1) -> Dict[str, Any]:
        """æ”¶é›†èŠå¤©äº¤äº’æ•°æ®"""
        start_date = timezone.now() - timedelta(days=days)
        
        messages = ChatMessage.objects.filter(
            session__user_id=user_id,
            created_at__gte=start_date
        ).select_related('session')
        
        data = {
            'total_messages': messages.count(),
            'user_messages': messages.filter(role='user').count(),
            'assistant_messages': messages.filter(role='assistant').count(),
            'total_tokens': sum(msg.tokens for msg in messages),
            'sessions': messages.values('session_id').distinct().count(),
            'topics': [],
            'emotions': [],
            'time_distribution': defaultdict(int)
        }
        
        # åˆ†æä¸»é¢˜å’Œæƒ…æ„Ÿ
        for msg in messages:
            if msg.emotion_snapshot:
                data['emotions'].append(msg.emotion_snapshot)
            
            # æ—¶é—´åˆ†å¸ƒ
            hour = msg.created_at.hour
            data['time_distribution'][hour] += 1
        
        return data
    
    @staticmethod
    def collect_learning_progress(user_id: int, days: int = 1) -> Dict[str, Any]:
        """æ”¶é›†å­¦ä¹ è¿›åº¦æ•°æ®"""
        start_date = timezone.now() - timedelta(days=days)
        
        progress_updates = LearningProgress.objects.filter(
            user_id=user_id,
            last_updated__gte=start_date
        ).select_related('node')
        
        data = {
            'nodes_updated': progress_updates.count(),
            'completed_nodes': progress_updates.filter(status='completed').count(),
            'in_progress_nodes': progress_updates.filter(status='in_progress').count(),
            'difficulty_distribution': defaultdict(int),
            'tag_distribution': defaultdict(int),
            'progress_details': []
        }
        
        for progress in progress_updates:
            node = progress.node
            data['difficulty_distribution'][node.difficulty] += 1
            
            for tag in node.tags:
                data['tag_distribution'][tag] += 1
            
            data['progress_details'].append({
                'node_title': node.title,
                'status': progress.status,
                'difficulty': node.difficulty,
                'tags': node.tags,
                'updated_at': progress.last_updated.isoformat()
            })
        
        return data
    
    @staticmethod
    def collect_knowledge_creation(user_id: int = None, days: int = 1) -> Dict[str, Any]:
        """æ”¶é›†çŸ¥è¯†ç‚¹åˆ›å»ºæ•°æ®"""
        start_date = timezone.now() - timedelta(days=days)
        
        queryset = KnowledgePoint.objects.filter(created_at__gte=start_date)
        # Note: KnowledgePoint doesn't have user field, so we collect all
        
        knowledge_points = queryset.all()
        
        data = {
            'total_points': knowledge_points.count(),
            'category_distribution': defaultdict(int),
            'importance_distribution': defaultdict(int),
            'source_distribution': defaultdict(int),
            'top_points': []
        }
        
        for kp in knowledge_points:
            data['category_distribution'][kp.category] += 1
            data['importance_distribution'][kp.importance] += 1
            data['source_distribution'][kp.source_type] += 1
        
        # è·å–é‡è¦çŸ¥è¯†ç‚¹
        top_points = knowledge_points.order_by('-importance', '-created_at')[:10]
        for kp in top_points:
            data['top_points'].append({
                'title': kp.title,
                'category': kp.category,
                'importance': kp.importance,
                'tags': kp.tags,
                'created_at': kp.created_at.isoformat()
            })
        
        return data


class KnowledgeExtractor:
    """çŸ¥è¯†ç‚¹æå–å™¨"""
    
    @staticmethod
    def extract_from_chat(chat_message: ChatMessage) -> List[Dict[str, Any]]:
        """ä»èŠå¤©æ¶ˆæ¯ä¸­æå–çŸ¥è¯†ç‚¹"""
        knowledge_points = []
        
        # ç®€å•çš„çŸ¥è¯†ç‚¹è¯†åˆ«è§„åˆ™
        content = chat_message.content
        
        # æŸ¥æ‰¾å®šä¹‰æ€§å†…å®¹
        if any(keyword in content.lower() for keyword in ['æ˜¯ä»€ä¹ˆ', 'å®šä¹‰', 'æ¦‚å¿µ', 'åŸç†']):
            knowledge_points.append({
                'title': f"æ¥è‡ªèŠå¤©çš„æ¦‚å¿µ: {content[:50]}...",
                'content': content,
                'category': 'æ¦‚å¿µå®šä¹‰',
                'importance': 3,
                'source_type': 'chat',
                'source_id': str(chat_message.id),
                'tags': ['èŠå¤©', 'æ¦‚å¿µ']
            })
        
        # æŸ¥æ‰¾æŠ€æœ¯å®ç°
        if any(keyword in content.lower() for keyword in ['å¦‚ä½•', 'å®ç°', 'æ–¹æ³•', 'æ­¥éª¤']):
            knowledge_points.append({
                'title': f"å®ç°æ–¹æ³•: {content[:50]}...",
                'content': content,
                'category': 'æŠ€æœ¯å®ç°',
                'importance': 4,
                'source_type': 'chat',
                'source_id': str(chat_message.id),
                'tags': ['èŠå¤©', 'å®ç°']
            })
        
        # æŸ¥æ‰¾é—®é¢˜è§£å†³
        if any(keyword in content.lower() for keyword in ['é—®é¢˜', 'é”™è¯¯', 'è§£å†³', 'è°ƒè¯•']):
            knowledge_points.append({
                'title': f"é—®é¢˜è§£å†³: {content[:50]}...",
                'content': content,
                'category': 'é—®é¢˜è§£å†³',
                'importance': 5,
                'source_type': 'chat',
                'source_id': str(chat_message.id),
                'tags': ['èŠå¤©', 'é—®é¢˜è§£å†³']
            })
        
        return knowledge_points
    
    @staticmethod
    def auto_extract_and_save(days: int = 1):
        """è‡ªåŠ¨æå–å¹¶ä¿å­˜çŸ¥è¯†ç‚¹"""
        start_date = timezone.now() - timedelta(days=days)
        
        # è·å–æœ€è¿‘çš„èŠå¤©æ¶ˆæ¯
        recent_messages = ChatMessage.objects.filter(
            created_at__gte=start_date,
            role='assistant'  # ä¸»è¦ä»åŠ©æ‰‹å›å¤ä¸­æå–çŸ¥è¯†ç‚¹
        ).select_related('session')
        
        extracted_count = 0
        for message in recent_messages:
            knowledge_points = KnowledgeExtractor.extract_from_chat(message)
            
            for kp_data in knowledge_points:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç±»ä¼¼çš„çŸ¥è¯†ç‚¹
                existing = KnowledgePoint.objects.filter(
                    source_type=kp_data['source_type'],
                    source_id=kp_data['source_id']
                ).first()
                
                if not existing:
                    knowledge_point = KnowledgePoint.objects.create(**kp_data)
                    
                    # åŒæ—¶æ·»åŠ åˆ°æœç´¢ç´¢å¼•
                    search_service.index_content(
                        'knowledge_point',
                        str(knowledge_point.id),
                        f"{knowledge_point.title}\n{knowledge_point.content}",
                        {
                            'category': knowledge_point.category,
                            'importance': knowledge_point.importance,
                            'tags': knowledge_point.tags
                        }
                    )
                    extracted_count += 1
        
        return extracted_count


class LearningReportGenerator:
    """å­¦ä¹ æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.collector = LearningDataCollector()
        self.extractor = KnowledgeExtractor()
    
    def generate_daily_report(self, user_id: int, date: datetime = None) -> LearningReport:
        """ç”Ÿæˆæ¯æ—¥å­¦ä¹ æŠ¥å‘Š"""
        if date is None:
            date = timezone.now().date()
        
        # æ”¶é›†æ•°æ®
        chat_data = self.collector.collect_chat_interactions(user_id, days=1)
        learning_data = self.collector.collect_learning_progress(user_id, days=1)
        knowledge_data = self.collector.collect_knowledge_creation(user_id, days=1)
        
        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_content = self._generate_report_content(chat_data, learning_data, knowledge_data)
        summary = self._generate_summary(chat_data, learning_data, knowledge_data)
        knowledge_points = self._extract_key_knowledge_points(knowledge_data)
        
        # åˆ›å»ºæŠ¥å‘Š
        report = LearningReport.objects.create(
            user_id=user_id,
            title=f"å­¦ä¹ æ—¥æŠ¥ - {date.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
            content=report_content,
            summary=summary,
            knowledge_points=knowledge_points,
            date_from=date,
            date_to=date
        )
        
        return report
    
    def _generate_report_content(self, chat_data: Dict, learning_data: Dict, knowledge_data: Dict) -> str:
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        content_parts = []
        
        # èŠå¤©äº¤äº’éƒ¨åˆ†
        content_parts.append("## ğŸ’¬ èŠå¤©äº¤äº’ç»Ÿè®¡")
        content_parts.append(f"- æ€»æ¶ˆæ¯æ•°: {chat_data['total_messages']}")
        content_parts.append(f"- ç”¨æˆ·æ¶ˆæ¯: {chat_data['user_messages']}")
        content_parts.append(f"- åŠ©æ‰‹å›å¤: {chat_data['assistant_messages']}")
        content_parts.append(f"- æ€»tokens: {chat_data['total_tokens']}")
        content_parts.append(f"- ä¼šè¯æ•°: {chat_data['sessions']}")
        
        # å­¦ä¹ è¿›åº¦éƒ¨åˆ†
        content_parts.append("\n## ğŸ“š å­¦ä¹ è¿›åº¦")
        content_parts.append(f"- æ›´æ–°èŠ‚ç‚¹æ•°: {learning_data['nodes_updated']}")
        content_parts.append(f"- å®ŒæˆèŠ‚ç‚¹: {learning_data['completed_nodes']}")
        content_parts.append(f"- è¿›è¡Œä¸­èŠ‚ç‚¹: {learning_data['in_progress_nodes']}")
        
        if learning_data['difficulty_distribution']:
            content_parts.append("- éš¾åº¦åˆ†å¸ƒ:")
            for difficulty, count in learning_data['difficulty_distribution'].items():
                content_parts.append(f"  - éš¾åº¦{difficulty}: {count}ä¸ªèŠ‚ç‚¹")
        
        # çŸ¥è¯†ç‚¹éƒ¨åˆ†
        content_parts.append("\n## ğŸ§  çŸ¥è¯†ç‚¹ç»Ÿè®¡")
        content_parts.append(f"- æ–°å¢çŸ¥è¯†ç‚¹: {knowledge_data['total_points']}")
        
        if knowledge_data['category_distribution']:
            content_parts.append("- ç±»åˆ«åˆ†å¸ƒ:")
            for category, count in knowledge_data['category_distribution'].items():
                content_parts.append(f"  - {category}: {count}ä¸ª")
        
        # é‡è¦çŸ¥è¯†ç‚¹
        if knowledge_data['top_points']:
            content_parts.append("\n## â­ é‡è¦çŸ¥è¯†ç‚¹")
            for i, point in enumerate(knowledge_data['top_points'][:5], 1):
                content_parts.append(f"{i}. **{point['title']}** (é‡è¦åº¦: {point['importance']})")
                content_parts.append(f"   ç±»åˆ«: {point['category']}")
        
        return "\n".join(content_parts)
    
    def _generate_summary(self, chat_data: Dict, learning_data: Dict, knowledge_data: Dict) -> str:
        """ç”ŸæˆæŠ¥å‘Šæ‘˜è¦"""
        summary_parts = []
        
        if chat_data['total_messages'] > 0:
            summary_parts.append(f"ä»Šæ—¥è¿›è¡Œäº†{chat_data['sessions']}æ¬¡å¯¹è¯")
            summary_parts.append(f"å…±{chat_data['total_messages']}æ¡æ¶ˆæ¯")
        
        if learning_data['completed_nodes'] > 0:
            summary_parts.append(f"å®Œæˆäº†{learning_data['completed_nodes']}ä¸ªå­¦ä¹ èŠ‚ç‚¹")
        
        if knowledge_data['total_points'] > 0:
            summary_parts.append(f"ç§¯ç´¯äº†{knowledge_data['total_points']}ä¸ªæ–°çŸ¥è¯†ç‚¹")
        
        if not summary_parts:
            return "ä»Šæ—¥æ— å­¦ä¹ æ´»åŠ¨è®°å½•"
        
        return "ï¼Œ".join(summary_parts) + "ã€‚"
    
    def _extract_key_knowledge_points(self, knowledge_data: Dict) -> List[Dict]:
        """æå–å…³é”®çŸ¥è¯†ç‚¹"""
        return [
            {
                'title': point['title'],
                'category': point['category'],
                'importance': point['importance'],
                'tags': point['tags']
            }
            for point in knowledge_data['top_points'][:10]
        ]


class LearningLoopService:
    """å­¦ä¹ é—­ç¯æœåŠ¡"""
    
    def __init__(self):
        self.report_generator = LearningReportGenerator()
        self.knowledge_extractor = KnowledgeExtractor()
    
    def run_daily_loop(self, user_id: int = None):
        """è¿è¡Œæ¯æ—¥å­¦ä¹ é—­ç¯"""
        results = {
            'reports_generated': 0,
            'knowledge_points_extracted': 0,
            'users_processed': 0
        }
        
        # è‡ªåŠ¨æå–çŸ¥è¯†ç‚¹
        extracted_count = self.knowledge_extractor.auto_extract_and_save(days=1)
        results['knowledge_points_extracted'] = extracted_count
        
        # ç”Ÿæˆå­¦ä¹ æŠ¥å‘Š
        if user_id:
            users = [User.objects.get(id=user_id)]
        else:
            # ä¸ºæ‰€æœ‰æ´»è·ƒç”¨æˆ·ç”ŸæˆæŠ¥å‘Š
            yesterday = timezone.now() - timedelta(days=1)
            active_users = User.objects.filter(
                chatsession__created_at__gte=yesterday
            ).distinct()[:100]  # é™åˆ¶å¤„ç†ç”¨æˆ·æ•°
            users = list(active_users)
        
        for user in users:
            try:
                report = self.report_generator.generate_daily_report(user.id)
                results['reports_generated'] += 1
                results['users_processed'] += 1
                
                # å°†æŠ¥å‘Šæ·»åŠ åˆ°æœç´¢ç´¢å¼•
                search_service.index_content(
                    'learning_report',
                    str(report.id),
                    f"{report.title}\n{report.summary}\n{report.content}",
                    {
                        'user_id': user.id,
                        'date_from': report.date_from.isoformat(),
                        'date_to': report.date_to.isoformat(),
                        'knowledge_points_count': len(report.knowledge_points)
                    }
                )
            except Exception as e:
                print(f"Error generating report for user {user.id}: {e}")
                continue
        
        return results
    
    def get_learning_insights(self, user_id: int, days: int = 7) -> Dict[str, Any]:
        """è·å–å­¦ä¹ æ´å¯Ÿ"""
        start_date = timezone.now() - timedelta(days=days)
        
        # è·å–å­¦ä¹ æŠ¥å‘Š
        reports = LearningReport.objects.filter(
            user_id=user_id,
            created_at__gte=start_date
        ).order_by('-created_at')
        
        # åˆ†æå­¦ä¹ æ¨¡å¼
        insights = {
            'total_reports': reports.count(),
            'knowledge_categories': defaultdict(int),
            'learning_trends': [],
            'recommendations': []
        }
        
        for report in reports:
            for kp in report.knowledge_points:
                insights['knowledge_categories'][kp.get('category', 'unknown')] += 1
            
            insights['learning_trends'].append({
                'date': report.date_from.isoformat(),
                'knowledge_points': len(report.knowledge_points),
                'summary': report.summary
            })
        
        # ç”Ÿæˆå»ºè®®
        if insights['total_reports'] > 0:
            top_category = max(insights['knowledge_categories'].items(), key=lambda x: x[1])[0]
            insights['recommendations'].append(f"æ‚¨åœ¨{top_category}æ–¹é¢å­¦ä¹ è¾ƒå¤šï¼Œå»ºè®®ç»§ç»­æ·±å…¥")
        
        return insights


# å…¨å±€å­¦ä¹ é—­ç¯æœåŠ¡å®ä¾‹
learning_loop_service = LearningLoopService()